layout.data.from=Data from {0}
layout.displayed=Displayed
all_variables=Complete table of variables
#English texts
job_comment=comment
job_ctime=time created
job_exectime=waiting until
job_etime=time eligible
job_expected_endtime=max till
job_headline=Job
job_hosts=exec host/cpu
job_first_host=host/cpu
job_mtime=time last changed state
job_start_time=start
job_planned_nodes=planned nodes
job_comp_time=completed
job_resource_nodes=resources
job_killedby=This job was killed by the planning system by signal {0}!
job_exited_error=This job ended with exit value {0}!
job_killed_over_limit=This job was killed by the planning system for exceeding the required limits. The reason should be in the job's comment:
job_memory_exceeded=Job exceeded reserved amount of memory!
job_exceeds_cputime=Job exceeded rerved number of CPUs!
job_underusing_cpus=Job used less than 3/4 of allocated CPUs!
jobs_C=completed
jobs_F=finished
jobs_comment=comment
jobs_cputimeused=CPU time
jobs_ctime=time created
jobs_planned_start=planned start
jobs_E=exiting
jobs_headline=Jobs
jobs_H=held
jobs_job=job
jobs_jobname=name
job_skoncil=not found (probably has already finished), no detailed information available
jobs_locked_groups=Reserved for following groups:
jobs_locked_users=Reserved for following users:
jobs_locked_hosts=Reserved for following submitting hosts:
jobs_mem_reserved=reserved mem
jobs_mem_used=used mem
jobs_ncpu=CPU
jobs_ngpu=GPU
jobs_Q=queued
jobs_queue=queue
jobs_R=running
jobs_S=suspended
jobs_state=state
jobs_T=being moved
jobs_user=user
jobs_waiting__headline=Jobs waiting in queue
jobs_waiting_reason=waiting reason
jobs_walltimeused=Wall time
jobs_W=waiting
jobs_X=transfered
jobs_last_job_ends_time=The last job should end before {0,date,d.M.yyyy} {0,time,HH:mm}.
job_variable_list=variables
job_submitdir=directory for stdout
job_workdir=working directory
# menu
menu_headline=PBSPro monitor
menu_jobslink=Jobs
menu_nodeslink=Nodes
menu_propslink=Properties
menu_queueslink=Queues
menu_userslink=Users
nodejsp_arch=architecture
nodejsp_assigned=assigned
nodejsp_available=total
nodejsp_cpu_num=CPU
nodejsp_ganglia=Information from Ganglia:
nodejsp_g_boottime=Boot time
nodejsp_g_cpu=CPUs
nodejsp_g_freedisk=Free disk
nodejsp_g_freemem=Free Memory
nodejsp_g_freeswap=Free swap
nodejsp_g_loadaverage=load average
nodejsp_g_load=CPU states
nodejsp_g_os=Operating system
nodejsp_g_reported=Reported at
nodejsp_g_sysclock=System clock
nodejsp_g_type=Machine type
nodejsp_ht=The machine has {0} CPUs with HyperThreading, i.e. the OS reports number of CPUs as {1} and the percentual load is computed from the number {1}.
nodejsp_jobs=job
nodejsp_maintenance=The machine does not accept new jobs, it is being prepared for maintenance because of "{0}".
nodejsp_planned_outage_note=The machine has planned outage because of "{0}".
nodejsp_planned_outage_start=The start is planned at {0,time,yyyy-MM-dd HH:mm:ss}.
nodejsp_planned_outage_end=The end is planned at {0,time,yyyy-MM-dd HH:mm:ss}.
nodejsp_name=name
nodejsp_not_found=Node not found ...
nodejsp_ntype=node type
nodejsp_numcpu=CPU planned
nodejsp_pbs=Information from PBSPro:
nodejsp_pbs_state=state in PBS
nodejsp_properties=properties
nodejsp_queue=dedicated to queue
nodejsp_reserved=The machine is reserved for "{0}".
nodejsp_state=state
nodejsp_usedcpu=reserved CPUs
nodejsp_usedgpu=reserved GPUs
nodejsp_usedmem=reserved memory
nodejsp_used_scratch_local=used /scratch
nodejsp_used_scratch_ssd=used /scratch.ssd
nodejsp_used_scratch_shared=used /scratch.shared

nodejsp_reserved_scratch_local=reserved /scratch
nodejsp_reserved_scratch_ssd=reserved /scratch.ssd
nodejsp_reserved_scratch_shared=reserved /scratch.shared

nodejsp_free_scratch_local=free space /scratch
nodejsp_free_scratch_ssd=free space /scratch.ssd
nodejsp_free_scratch_shared=free space /scratch.shared

nodejsp_scratch_size=space in SCRATCHDIR
nodejsp_scratch_size_local=size of /scratch
nodejsp_scratch_size_ssd=size of/scratch.ssd
nodejsp_scratch_size_shared=size of /scratch.shared
nodejsp_on_node=on this node

nodejsp_cloud=This machine is dedicated for creation of new virtual machines on demand.

nodes_jsp_storages=Storages
nodes_jsp_storages_dir=directory
nodes_jsp_storages_usage=usage
nodes_jsp_storages_size=size
nodes_jsp_storages_used=used
nodes_jsp_storages_free=free
nodes_jsp_storages_totalSize=Total storage space in MetaCentrum
nodes_jsp_storages_totalUsed=Total used
nodes_jsp_storages_totalFree=Total free
nodes_jsp_storage_scratch_title=Scratch
nodes_jsp_storage_scratch_text=Each computing node has fast local storage, its size is displayed in detail of each node.
nodes_jsp_storage_scratch_warn=Use scratch <b>during computation only</b>. Data are deleted 14 days after their job ended.
nodes_jsp_storage_arrays_title=Disk arrays
nodes_jsp_storage_arrays_text=Disk arrays consist of many connected disks. A file is stored on multiple disks, moreover its data are stored redundantly. When compared with a single disk, disk array has higher read and write performance, and is more robust against data loss.
nodes_jsp_storage_arrays_warn=Use disk arrays <b>for storing data between jobs</b>.
nodes_jsp_storage_hsm_title=Hierarchical storages
nodes_jsp_storage_hsm_text=Hierarchical storages consist of several layers of storage media, where lower layers have bigger capacity, but slower access times. Data are moved among layers based on their use. The first layer is a disk array. Other layers are made of MAIDs (massive array of idle drives), or magnetic tape libraries.
nodes_jsp_storage_hsm_warn=Use hierarchical storages for storing <b>data that you do not use anymore, but you may need them in the future</b>.
nodes_jsp_person_quotas_before=Try your
nodes_jsp_person_quotas_link=personal view of storages
nodes_jsp_personlink_before=Try your
nodes_jsp_personlink_link=personal view of computing machines
nodes_jsp_computing_machines=Computing machines
nodelistjsp_headline=Nodes state from Ganglia
nodes_jsp_barvy=Colors for states
nodesjsp_cluster=cluster name
nodesjsp_cpucount=CPU
nodes_jsp_lock_legend=machine is assigned to a specific queue
nodesjsp_nodecount=nodes
nodesjsp_nodelistlink=Display more detailed list from Ganglia
nodesjsp_nodes=Nodes
nodesjsp_personlink=Personal view
nodesjsp_phys_machines=Physical size of machines
nodesjsp_server=server
nodesjsp_state_down=down
nodesjsp_state_free=free
nodesjsp_state_job-busy=job busy
nodesjsp_state_job-exclusive=job exclusive
nodesjsp_state_job-sharing=job sharing
nodesjsp_state_maintenance_busy=maintenance busy
nodesjsp_state_maintenance=maintenance
nodesjsp_state_offline=offline
nodesjsp_state_partialy-free=partialy free
nodesjsp_state_reserved=reserved
nodesjsp_state_state-unknown=state unknown
nodesjsp_state_test=preemption
nodesjsp_totalCPUs=total CPUs
nodesjsp_totaljobs=total jobs
nodesjsp_total=total
nodes_jsp_upozorneni=Due to the advanced stage in virtualization of MetaCentrum machines, this page no longer provides reliable information about physical machines. We are preparing <a href="/pbsmon2/">a new application</a>, that will display physical machines. Untill it is finished please bear in mind that machines with suffix <b>-2</b> are second virtual machines located on the same physical machine as machines with suffix <b>-1</b>.
nodesjsp_version=version
nodes_jsp_waiting_jobs={0,choice,0#no jobs|1#1 job|1<{0} jobs}
nodes_jsp_waiting_text1=There are
nodes_jsp_waiting_text3=in queues waiting to be executed.
nodesjsp_wapversion=WAP version
#page job.jsp
#page jobs.jsp
#page node.jsp
#page nodelist.jsp
#page nodes.jsp
#page propsjsp_jsp
person_accessible_queues=Queues accessible for user
person_dost_volnych=The requirement is {0,choice,0#no machine|1#1 machine|1<{0} machines}, and {1,choice,0#no such machine|1#1 such machine|1<{1} such machines} are free, out of {2,choice,1#1 machine|1<{2} machines} matching the requirements. The job may be started immediately. If it is not, do not panic, there may be a multi-CPU\u00A0job in the queue ahead of jor job, and the planner may be collection free CPUs for it.
person_dotaz=Selection
person_headline=Personal view
person_hosts_h3=Nodes accessible to user "{0}" from the queue "{1}"
person_jobs=Jobs of user "{0}"
person_malo_potencialnich=The requirement is {0,choice,0#no machine|1#1 machine|1<{0} machines}, but there {1,choice,0#is no machine|1#is only 1 machine|1<are only {1} machines} on which you have an account, the job will never be started !
person_malo_volnych=The requirement is {0,choice,0#no machine|1#1 machine|1<{0} machines}, but {1,choice,0#no such machine|1#only 1 such machine|1<only {1} such machines} are available, the job will wait in the queue.
person_nalezt=Find machines mathing the resource specification
person_potencialni=V\u0161echny stroje odpov\u00EDdaj\u00EDc\u00ED po\u017Eadavku
person_sestavovac=Command qsub refining
person_tedvolne=Machines available right now
person_text=This page shows a personal view of the PBS system for the user <b>{0}</b>, i.e. queues and computing nodes accessible by the user.
person_vlastnosti_stroju=Machine properties
person_vypis_uloh=list of jobs
person_vysledek=Result
person_vyznam=For properties meaning see
person_zadne_stroje_ve_fronte=You have no accounts on machines in this queue.
person_zadne_stroje_ve_fronte_prazdna_fronta=The queue does not contain any machines right now.
person_zridte_ucty=If you can, get accounts on the following machines:
person.qsub.queue.not.for.walltime=Walltime is outside of queue limits !
person.qsub.no.queue.for.walltime=No queue matches required walltime !


props_headline=Assigned properties
props_node2prop_node=Node
props_node2prop=Properties assigned to nodes
props_node2prop_prop=Properties
props_prop2node_node=Nodes
props_prop2node=Nodes assigned to properties
props_prop2node_prop=Property
props_volnych_CPU=CPU free
props_volnych=Free
q_headline=Queue
q_nodes=Available nodes
q_machines=Available machines
q_machines_cpus=The queue has access to {0} nodes located at {1} machines with total of {2} CPUs.

q_maintenance_txt=Special queue marking machines in maintenance
q_preempt_ncbr_txt=Queue allowing to preempt jobs on NCBR machines and start an urgent job in the -2 domains. Use wisely.
q_reserved_txt=Special queue marking machines temporarily reserved
q_pa177_txt=Queue for students of the PA177 course at Masaryk University
q_xentest_txt=Experimental queue for second virtual machines
q_globus_txt=Special queue for jobs coming from the Globus system
q_priority_txt=Special queue for administrative purposes
q_interactive_txt=Experimental queue for interactive parallel jobs with preemption
q_iti_txt=Queue for users from ITI\u00A0- Institute of Theoretical Informatics (Univ. of West Bohemia)
q_orca16g_txt=Queue for jobs needing 16GB\u00A0memory on the orca cluster
q_orca_txt=Queue for jobs on the orca cluster
q_ncbr_txt=Queue for users from  NCBR - National Center for Biomolecular Research (MU)
q_long_txt=Common queue for jobs lasting up to 30 days
q_cpmd_txt=Queue for users from  NCBR using the CPMD application
q_short_txt=Common queue for jobs lasting up to 2 hours
q_privileged_txt=Queue for users qith at least 3 publications acknowledging the MetaCentrum or who helped to write guides or documentation for computing applications
q_zsc_txt=Queue for users from ZSC - West Bohemian Supercomputing Center
q_normal_txt=Common queue for jobs lasting up to 24 hours
q_quark_txt=Queue for users of the Quark cluster
q_mikroskop_txt=Queue for processing microscope images on the Quark cluster
q_default_txt=Routing queue sending jobs to queues short and normal
q_jcu_txt=Queue for users from JCU - University of South Bohemia
q_jcu2_txt=Queue for users from JCU - University of South Bohemia, faster machines than in queue "jcu"
q_preempt_txt=Queue enabling preemption of runing jobs. Only for jobs with at least 10 CPU. Contact us if you have such jobs.
q_preemptible_txt=Queue allowing normal users to use machines where their owners can do preemption of jobs. The jobs can be delayed for up to 30 days.
q_lsd_txt=Queue for users from LSD\u00A0- Laboratory of Searching nad Dialogue (MU)
q_backfill_txt=Queue for backfilling jobs - jobs with low priority, jobs which may be terminated when resources are needed, jobs which use at most one machine, but there can be many jobs in this queue.
q_feec_txt=Queue for users from FEEC, University of Technology, Brno
q_loslab_txt=Queue for users from Loschmidt Laboratories (MU)
q_mufin_txt=Queue for users from the project MUFIN (FI MU), priority access to manwe3+4
q_ncbr_medium_txt=Queue for users from NCBR, for jobs lasting up to 5 days
q_ncbr_long_txt=Queue for users from NCBR, for jobs lasting up to 30 days
q_gpu_txt=Queue for jobs computed on GPU
q_private_txt=Testing queue for administrators
q_debian6_txt=Queue for jobs requiring OS Debian 6.0
q_q_2h_txt=Queue for jobs lasting from 0 up to 2 hours
q_q_4h_txt=Queue for jobs lasting from 2 up to 4 hours
q_q_1d_txt=Queue for jobs lasting from 4 up to 24 hours
q_q_2d_txt=Queue for jobs lasting from 1 up to 2 days
q_q_4d_txt=Queue for jobs lasting from 2 up to 4 days
q_q_1w_txt=Queue for jobs lasting from 4 days up to 1 week
q_q_2w_txt=Queue for jobs lasting from 1 week up to 2 weeks
q_q_2w_plus_txt=Queue for jobs lasting from 2 weeks up to 2 months
q_p2ptest_txt=Special queue for tests of peer-to-peer communication among PBS servers
q_ncbr_single_txt=Queue for single-node jobs in NCBR
q_gpu_long_txt=Queue for long time computations on GPU
q_debian7_txt=Queue for tests of Debian 7 installation
q_monitoring_txt=Special queue for PBS monitoring
q_MetaSeminar_txt=Queue for seminar
q_ops_txt=Special queue for administrative purposes
q_batch_txt=Fictional queue, this scheduler uses plan-based approach instead of queues

#queue.jsp
queues_completed=completed
queues_headline=Queues
queues_jobs=jobs
#queues.jsp
queues_max_running=max
queues_max_user_run=max jobs per user
queues_max_user_cpus=max CPUs per user
queues_priority=Priority
queues_queued=queued
queues_queue=queue
queues_rprop=required property
queues_running=running 
queues_timelimit=time limits
queues_total=total

tabled_variables=Basic info
user_headline=User
#user.jsp
users_headline=Users
users_jobs_fairshare=fairshare
users_jobs_count=job count
users_jobs_ncpu=CPU count
users_jobs_othercpu=other CPU
users_jobs_otherjob=other jobs
users_jobs_runjob=running jobs
users_jobs_usedcpu=used CPU
#users.jsp
user_skoncil=has currently no running or planed jobs
warning1_title=Problem
warning2_label=Warning
warning3_label=OK
index.titul=The state of the MetaCentrum
jobs_celkem=total
jobs_pocet=number of jobs
jobs_queued_duvod=reason for waiting
jobs_queued.titul=Jobs waiting in queues
jobs.titul=Jobs
jobs.all.titul=All jobs
job.titul=Job {0}
mapping.titul=Mapping of virtual machines to physical machines
nodes_jsp_celkem_cpu=Total CPUs in MetaCentrum
nodes_jsp_nezarazene=Unassigned machines
nodes.titul=Physical machines
node.titul=PBS node {0}
props_group2node_group=properties
props_group2node=machines with equal properties
props.titul=Machine properties
queues.titul=Queues
queue.titul=Queue {0}
server_arien.ics.muni.cz=Production environment
server_arien-pro.ics.muni.cz=New planner PBSPro
server_skirit-f.ics.muni.cz=Old environment
server_torque1.ics.muni.cz=Experimental environment
server_wagap.cerit-sc.cz=Environment of center CERIT-SC
server_wagap-devel.cerit-sc.cz=Experimental plan-based scheduler without queues
servers.titul=Servers
user_jsp_hlaska_kdo=User <b>{0}</b> from organization <b>{1}</b> belongs to research group <b>{2}</b> and has a MetaCentrum account expiring on <b>{3,date,medium}</b>.
user_jsp_hlaska_propocital= Computed  <b>{0,choice,0#no jobs|1#1 job|1<{0} jobs}</b> with the total CPU time of <b>{1,number, #.#} days</b>.
user_jsp_hlaska_publikace= Produced <b>{0,choice,0#no publication|1#1 publication|1<{0} publications}</b> with acknowledgement to the MetaCentrum and <b>{1,choice,0#no publication|1#1 publication|1<{1} publications}</b> with acknowledgement to the CERIT-SC.
user_jsp_ulohy_v_PBS=Jobs in PBS
users_jobs_cpusOther=other
users_jobs_cpusStateC=completed
users_jobs_cpusStateQ=queued
users_jobs_cpusStateR=running
users_jobs_cpusTotal=total
users_jobs_jobsOther=other
users_jobs_jobsStateC=completed
users_jobs_jobsStateQ=queued
users_jobs_jobsStateR=running
users_jobs_jobsTotal=total
users_jsp_celkem=The total number of users with jobs is {0}.
user.titul=User {0}
mapping_jsp_fyzicke=Physical machines
mapping_jsp_virtualni=Virtual machines


memi.errorMessage=The size of requested memory in the 'mem' parameter ({1}) must be a positive number.
scratch.errorMessage=The size of requested disk free space in the 'scratch' parameter ({1}) must be a positive number.

machine.titul=Physical machine {0}
machine_jsp_ma_pbs_node=The physical machine has PBS node
machine_jsp_ma_virts=The physical machine has {0,choice,0#no virtual machines.|1#1 virtual machine:|1<{0} virtual machines:}.
machine_jsp_physical=Physical machine
machine_jsp_virtual=PBS node
machine_jsp_neni_v_pbs=is not in PBS
machine_jsp_in_cloud=Physical machine {0} participates in the OpenNebula cloud.
machine_jsp_cloud_vms=Physical machine hosts the following virtual machines:

node_detail_fronty=Accessible thru these queues on the PBS server
queue_line_tag_locked=Reserved for
nodesjsp_state_blue=busy or reserved
nodesjsp_state_gray=maintenance and other states

queues_list_urceni_front=Purposes of queues
queues_list_locked_for_users=for users
queues_list_locked_for_groups=for groups
queues_list_locked_for_hosts=for hosts
queues_list_popis=description
queues_list_omezeni=access restriction
queues_list_urceni_pokec1=For common usage, do not specify any queue, just the required walltime.
queues_list_urceni_pokec2=Special queues are used for marking nodes, not for jobs. Experimental queues are not in use. Users salvet, ruda, mulac, makub, xhejtman, dexter, jeronimo, vchlum, ali, bodik are administrators of MetaCentrum.
queues_list_queueus_nodes_headline=Nodes in queues
queues_list_no_nodes_in_queue=this queue has no nodes

jobs_pocet_cpu=CPU count
machine_jsp_fyzicky_frontend=Machine <b>{0}</b> is cluster frontend, i.e. users can log in to the machine directly.
machine_jsp_virtual_frontend=(cluster frontend)
stats.titul=Overview
stats_vyuziti=usage
stats_cluster=cluster of {0} machines with {1,choice,1#1 CPU|1<{1} CPUs}
stats_samostatny_stroj=single machine with {0} CPUs

node_detail_tag_outages=Recorded outages from normal operation
node_detail_tag_typ=type
node_detail_tag_start=start
node_detail_tag_end=end
node_detail_tag_comment=comment
node_detail_tag_none_found=none found
node_detail_max_walltime=This node accepts only jobs with maximum walltime <b>{0}</b>.
node_detail_min_walltime=This node accepts only jobs with minimum walltime <b>{0}</b>.


jobs.show.zobrazit.moje.ulohy=Show my jobs
jobs.show.zobrazit.vsechny.ulohy=Show all jobs
jobs.show.vsechny.komentar=(can be very long list !)
jobs.show.zobrazit.ulohy.pro.uzivatele=Show jobs for a selected user
jobs.show.zobrazit.varovani=Show warnings about jobs in suspicious states
user.rok=year
user.cpudny=CPUdays

resource.titul.cluster=Cluster {0}
resource.titul.machine=Machine {0}
resource.owner=owner
resource.cluster.nadpis=Cluster {0} contains {1,choice,|1# 1 node|2< {1} nodes}, each of the nodes has the following hardware specification:
resource.comment=comment

machine_jsp_soucast_clusteru1=The machine is in the cluster
machine_jsp_soucast_clusteru2=whose nodes have the following specification:
machine_jsp_smp_stroj1=The machine is not in a cluster, it is an individual SMP machine
machine_jsp_smp_stroj2=with the following specification:

hardware.titul=Hardware
hardware.uzlu={0,choice,|1# 1 node|2< {0} nodes}

cloud.titul=Cloud
jobs_queued_byqueuefalse=Server has setting by_queue=false.
jobs_queued_byqueuetrue=Server has setting by_queue=true.

nodes_jsp_gpu_title=GPU
nodejsp_status_message=message
job_scheduled_nodespec=reserved resources
job_nodespec_node=node
cloud_jsp_celkem_cpu=Total CPU in cloud:
cloud_jsp_headline_physical_machines=Overview of physical machines in the cloud
cloud_jsp_headline_vm_assignment=Assignment of VMs to physical hosts
cloud_state_blue=fully occupied
cloud_jsp_barvy=machine states
cloud_state_nebulapbshost_full= fully occupied by PBS node
cloud_state_nebulapbshost_partial= partially occupied by PBS node

